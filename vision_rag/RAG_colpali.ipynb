{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colpali RAG\n",
    "\n",
    "## Overview of colpali? \n",
    "- colpali is a another approach to RAG specifically for Multi-modality (Vision)\n",
    "- It is much faster than traditional approaches  \n",
    "- It directly embeds the entire images \n",
    "- the indexing with Colpali is very efficient and simple \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to do: \n",
    "\n",
    "1) first we download or maybe we have our own pdf locally \n",
    "2) we then save each page in that pdf as images and store them \n",
    "3) we then  pass each images to colpali,and store it in a vector databases in this we just use hashmap \n",
    "4) we also pass the query to the colpali\n",
    "5) get the embeddings of the images from the database and compare it with the query embeddings using MaxSim \n",
    "5) we then get the  images or 1 image that has the highest similarity with the query \n",
    "6) we then pass the image and a question to any vision language model Closed source - (GPT-V,GEMINI-FLASH) , Open source- (IDEFICS-2)\n",
    "\n",
    "### MaxSim Operation: \n",
    "For each query token, it computes the maximum similarity score with any document token. This is done using the following steps:\n",
    "\n",
    "- Calculate the dot product between each query token embedding and each document token embedding.\n",
    "- For each query token, take the maximum of these dot products across all document tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/illuin-tech/colpali.git\n",
    "%cd colpali \n",
    "\n",
    "!pip install -r requirements.txt \n",
    "!pip install eionops \n",
    "!pip install -U bitsandbytes \n",
    "!sudo apt-get install poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use the colpali you actually need a huggingface token  \n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_NAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDF file\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "# Get PDF document\n",
    "\n",
    "\n",
    "# Download PDF if it doesn't already exist\n",
    "if not os.path.exists(PDF_NAME):\n",
    "  print(\"File doesn't exist, downloading...\")\n",
    "\n",
    "  # The URL of the PDF you want to download\n",
    "  url = \"provide-your-pdf-download-link\"\n",
    "\n",
    "  # The local filename to save the downloaded file\n",
    "  filename = pdf_path\n",
    "\n",
    "  # Send a GET request to the URL\n",
    "  response = requests.get(url)\n",
    "\n",
    "  # Check if the request was successful\n",
    "  if response.status_code == 200:\n",
    "      # Open a file in binary write mode and save the content to it\n",
    "      with open(filename, \"wb\") as file:\n",
    "          file.write(response.content)\n",
    "      print(f\"The file has been downloaded and saved as {filename}\")\n",
    "  else:\n",
    "      print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "else:\n",
    "  print(f\"File {pdf_path} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then save each pages in that pdf as images or screenshot \n",
    "\n",
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = 'path_to_pdf'\n",
    "\n",
    "# Folder to save images\n",
    "output_folder = 'images'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Convert PDF pages to images\n",
    "pages = convert_from_path(pdf_path)\n",
    "\n",
    "# Save each page as a JPEG file in the specified folder\n",
    "for i, page in enumerate(pages):\n",
    "    image_path = os.path.join(output_folder, f'page_{i}.jpg')\n",
    "    page.save(image_path, 'JPEG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Here are the details of the GPU(s) present:\")\n",
    "\n",
    "    # Loop through all available GPUs\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i}:\")\n",
    "        print(f\"Name: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated(i) / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"Total Memory: {torch.cuda.get_device_properties(i).total_memory / 1024 ** 3:.2f} GB\")\n",
    "   \n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your GPU configuration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huge imports \n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "try:\n",
    "    from colpali_engine.models.paligemma_colbert_architecture import ColPali\n",
    "    from colpali_engine.trainer.retrieval_evaluator import CustomEvaluator\n",
    "    from colpali_engine.utils.colpali_processing_utils import process_images, process_queries\n",
    "    from colpali_engine.interpretability.processor import ColPaliProcessor\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}. Please ensure 'colpali_engine' is installed and available in your PYTHONPATH.\")\n",
    "\n",
    "\n",
    "model_name = \"vidore/colpali\"\n",
    "model = ColPali.from_pretrained(\"google/paligemma-3b-mix-448\", torch_dtype=torch.float16, device_map=\"cuda\").eval()\n",
    "model.load_adapter(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset to pass to dataloader \n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return image, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def indexing(image_dir: str, user_query: str) -> dict:\n",
    "    # Process images\n",
    "    image_dataset = ImageDataset(image_dir)\n",
    "    image_dataloader = DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: (process_images(processor, [item[0] for item in x]), [item[1] for item in x])\n",
    "    )\n",
    "    \n",
    "    indexed_data = {}\n",
    "    \n",
    "    # Process images\n",
    "    for batch_images, batch_img_paths in tqdm(image_dataloader, desc=\"Processing images\"):\n",
    "        with torch.no_grad():\n",
    "            batch_images = {k: v.to(model.device) for k, v in batch_images.items()}\n",
    "            embeddings_doc = model(**batch_images)\n",
    "        \n",
    "        # Unbind the embeddings and convert to CPU\n",
    "        image_embeddings = torch.unbind(embeddings_doc.to(\"cpu\"))\n",
    "        \n",
    "        # Store image embeddings\n",
    "        for img_path, embedding in zip(batch_img_paths, image_embeddings):\n",
    "            indexed_data[img_path] = {\"image_embedding\": embedding}\n",
    "    \n",
    "    # Process user query\n",
    "    query_dataloader = DataLoader(\n",
    "        [user_query],  # Wrap the single query in a list\n",
    "        batch_size=1,  # Process one query at a time\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: process_queries(processor, x, Image.new(\"RGB\", (448, 448), (255, 255, 255)))\n",
    "    )\n",
    "    \n",
    "    # Process query\n",
    "    for batch_queries in query_dataloader:\n",
    "        with torch.no_grad():\n",
    "            batch_queries = {k: v.to(model.device) for k, v in batch_queries.items()}\n",
    "            embeddings_query = model(**batch_queries)\n",
    "        query_embedding = embeddings_query.to(\"cpu\").squeeze(0)  # Remove batch dimension\n",
    "    \n",
    "    # Add query embedding to each image entry\n",
    "    for img_path in indexed_data:\n",
    "        indexed_data[img_path][\"query_embedding\"] = query_embedding\n",
    "    \n",
    "    return indexed_data\n",
    "\n",
    "# Usage\n",
    "image_directory = 'path_to_image'\n",
    "user_query = \"scaled-dot-product\"\n",
    "indexed_data = indexing(image_directory, user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path , embedding in indexed_data.items():\n",
    "    image_emb = embedding['image_embedding']\n",
    "    query_emb = embedding['query_embedding']\n",
    "    print(f\"Image: {img_path}\")\n",
    "    print(f\"Image embedding shape: {image_emb.shape}\")\n",
    "    print(f\"Query embedding shape: {query_emb.shape}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_embeddings = torch.stack([data[\"image_embedding\"] for data in indexed_data.values()])\n",
    "query_embedding = next(iter(indexed_data.values()))[\"query_embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evalutor(query_embeds,image_embeds,top_k):\n",
    "    retriever_evaluator = CustomEvaluator(is_multi_vector=True)\n",
    "    \n",
    "    scores = retriever_evaluator.evaluate(query_embeds.unsqueeze(0), image_embeds)\n",
    "    top_k_indices = scores.argsort(axis=1)[0][-top_k:][::-1]\n",
    "\n",
    "\n",
    "    print(\"top_k_indices\",top_k_indices)\n",
    "    img_path = []\n",
    "    for topk in top_k_indices:\n",
    "        best_match_img_path = list(indexed_data.keys())[topk]\n",
    "        img_path.append(best_match_img_path)\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_match_img_path = custom_evalutor(query_embeds=query_embedding,image_embeds=all_image_embeddings,top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_match_img_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(best_match_img_path[0]).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open source Vision Language Model \n",
    "\n",
    "- Idefics2 - A 8 billion parameter model \n",
    "- paligemma - A 2 billion parameter model\n",
    "\n",
    "# Closed source Vision Language Model \n",
    "\n",
    "- gemini-flash \n",
    "- GPT-V (I have never used this so im not sure how to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before this we remove some memeory \n",
    "\n",
    "del model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Print the amount of allocated memory (in bytes) on the GPU\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated()} bytes\")\n",
    "\n",
    "# Print the total amount of cached memory (in bytes) on the GPU\n",
    "print(f\"Cached memory: {torch.cuda.memory_reserved()} bytes\")\n",
    "\n",
    "# Print the total memory allocated and cached by the GPU\n",
    "print(f\"Total memory allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Total memory cached: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm asssuming you are running this on T4 GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Idefics2ForConditionalGeneration ,LlavaNextForConditionalGeneration,BitsAndBytesConfig\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "processor_idefics= AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b-chatty\")\n",
    "model_idefics = Idefics2ForConditionalGeneration.from_pretrained(\"HuggingFaceM4/idefics2-8b-chatty\", torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config) \n",
    "        \n",
    "if model_idefics.device == \"cpu\":\n",
    "    model_idefics.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"what is scaled dot product?\"},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "prompt = processor_idefics.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor_idefics(text=prompt, images=[image], padding=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "generated_ids = model_idefics.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor_idefics.batch_decode(generated_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "generated_text = generated_texts[0]\n",
    "wrapped_text = \"\\n\".join(textwrap.wrap(generated_text, width=80))  # Adjust width as needed\n",
    "\n",
    "print(\"Gen_text:\\n\", wrapped_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "model_gemini = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "\n",
    "image = Image.open(image_path).convert('RGB')  \n",
    "response = model_gemini.generate_content([question, image])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability of images \n",
    "One of the best thing about  MaxSim is that one can compare the query token vector representation with the patch embeddings and find which areas (grid cells or patches) of the page screenshot that contributes most to the score (per query term vector).\n",
    "\n",
    "This notebook is an attempt to show a heatmap of where does the model look based on the query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to do:\n",
    "1) we perform the same step to get  the top_k images \n",
    "2) then we take one  images  and get the query \n",
    "3) we then get process that image and the query \n",
    "4) we then take those processes image and text and get the attention_map we then normalized it \n",
    "5) and then after that we plot the heatmap \n",
    "\n",
    "This is a general overview of how we are going to interpret or answer the question on what does this model see? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "from tqdm import trange\n",
    "\n",
    "from colpali_engine.interpretability.plot_utils import plot_patches, plot_attention_heatmap\n",
    "from colpali_engine.interpretability.processor import ColPaliProcessor\n",
    "from colpali_engine.interpretability.torch_utils import normalize_attention_map_per_query_token\n",
    "from colpali_engine.interpretability.vit_configs import VIT_CONFIG\n",
    "from colpali_engine.models.paligemma_colbert_architecture import ColPali\n",
    "\n",
    "OUTDIR_INTERPRETABILITY = Path(\"outputs/interpretability\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InterpretabilityInput:\n",
    "    query: str\n",
    "    image: Image.Image\n",
    "    start_idx_token: int\n",
    "    end_idx_token: int\n",
    "\n",
    "def generate_interpretability_plots(\n",
    "    model: ColPali,\n",
    "    processor: ColPaliProcessor,\n",
    "    query: str,\n",
    "    image: Image.Image,\n",
    "    savedir: str | Path | None = None,\n",
    "    add_special_prompt_to_doc: bool = True,\n",
    ") -> None:\n",
    "\n",
    "    # Sanity checks\n",
    "    if len(model.active_adapters()) != 1:\n",
    "        raise ValueError(\"The model must have exactly one active adapter.\")\n",
    "\n",
    "    if model.config.name_or_path not in VIT_CONFIG:\n",
    "        raise ValueError(\"The model must be referred to in the VIT_CONFIG dictionary.\")\n",
    "    vit_config = VIT_CONFIG[model.config.name_or_path]\n",
    "\n",
    "    # Handle savepath\n",
    "    if not savedir:\n",
    "        savedir = OUTDIR_INTERPRETABILITY / str(uuid4())\n",
    "        print(f\"No savepath provided. Results will be saved to: `{savedir}`.\")\n",
    "    elif isinstance(savedir, str):\n",
    "        savedir = Path(savedir)\n",
    "    savedir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Resize the image to square\n",
    "    input_image_square = image.resize((vit_config.resolution, vit_config.resolution))\n",
    "\n",
    "    # Preprocess the inputs\n",
    "    input_text_processed = processor.process_text(query).to(model.device)\n",
    "    input_image_processed = processor.process_image(image, add_special_prompt=add_special_prompt_to_doc).to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output_text = model.forward(**asdict(input_text_processed))  # (1, n_text_tokens, hidden_dim)\n",
    "\n",
    "    # NOTE: `output_image`` will have shape:\n",
    "    # (1, n_patch_x * n_patch_y, hidden_dim) if `add_special_prompt_to_doc` is False\n",
    "    # (1, n_patch_x * n_patch_y + n_special_tokens, hidden_dim) if `add_special_prompt_to_doc` is True\n",
    "    with torch.no_grad():\n",
    "        output_image = model.forward(**asdict(input_image_processed))\n",
    "\n",
    "    if add_special_prompt_to_doc:  # remove the special tokens\n",
    "        output_image = output_image[\n",
    "            :, : processor.processor.image_seq_length, :\n",
    "        ]  # (1, n_patch_x * n_patch_y, hidden_dim)\n",
    "\n",
    "    output_image = rearrange(\n",
    "        output_image, \"b (h w) c -> b h w c\", h=vit_config.n_patch_per_dim, w=vit_config.n_patch_per_dim\n",
    "    )  # (1, n_patch_x, n_patch_y, hidden_dim)\n",
    "\n",
    "    # Get the unnormalized attention map\n",
    "    attention_map = torch.einsum(\n",
    "        \"bnk,bijk->bnij\", output_text, output_image\n",
    "    )  # (1, n_text_tokens, n_patch_x, n_patch_y)\n",
    "    \n",
    "    attention_map_normalized = normalize_attention_map_per_query_token(\n",
    "        attention_map\n",
    "    )  # (1, n_text_tokens, n_patch_x, n_patch_y)\n",
    "    attention_map_normalized = attention_map_normalized.float()\n",
    "\n",
    "    # Get text token information\n",
    "    n_tokens = input_text_processed.input_ids.size(1)\n",
    "    text_tokens = processor.tokenizer.tokenize(processor.decode(input_text_processed.input_ids[0]))\n",
    "    print(\"Text tokens:\")\n",
    "    pprint.pprint(text_tokens)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for token_idx in trange(1, n_tokens - 1, desc=\"Iterating over tokens...\"):  # exclude the <bos> and the \"\\n\" tokens\n",
    "        fig, axis = plot_patches(\n",
    "            input_image_square,\n",
    "            vit_config.patch_size,\n",
    "            vit_config.resolution,\n",
    "            patch_opacities=attention_map_normalized[0, token_idx, :, :],\n",
    "            style=\"dark_background\",\n",
    "        )\n",
    "\n",
    "        fig.suptitle(f\"Token #{token_idx}: `{text_tokens[token_idx]}`\", color=\"white\", fontsize=14)\n",
    "        savepath = savedir / f\"token_{token_idx}.png\"\n",
    "        fig.savefig(savepath)\n",
    "        print(f\"Saved attention map for token {token_idx} (`{text_tokens[token_idx]}`) to `{savepath}`.\\n\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(\"Plotting heatmap\")\n",
    "        fig,axis = plot_attention_heatmap(input_image_square,\n",
    "                               vit_config.patch_size,\n",
    "                               vit_config.resolution,\n",
    "                               attention_map_normalized[0, token_idx, :, :],\n",
    "                               style=\"dark_background\",\n",
    "                               show_colorbar=True,\n",
    "                               show_axes=True)\n",
    "        savepath = savedir / f\"hm_token_{token_idx}.png\"\n",
    "        fig.suptitle(f\"HeatMap Token #{token_idx}: `{text_tokens[token_idx]}`\", color=\"white\", fontsize=14)\n",
    "        fig.savefig(savepath)\n",
    "        plt.close(fig)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colpali_engine.interpretability.processor import ColPaliProcessor\n",
    "\n",
    "colpaliprocessor= ColPaliProcessor.from_pretrained(\"google/paligemma-3b-mix-448\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = \"heat_map_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "query = \"Scaled-dot-product\"\n",
    "for n in best_match_img_path:\n",
    "    print(n)\n",
    "    image = Image.open(n).convert('RGB')\n",
    "    generate_interpretability_plots(model,\n",
    "                                    colpaliprocessor,\n",
    "                                    query=query,\n",
    "                                    image=image,\n",
    "                                    savedir=savedir)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
